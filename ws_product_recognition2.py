# -*- coding: utf-8 -*-
"""WS_Product_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PuS4XkajPI4MLtB5vKqkbrpq4bnRqJx9
"""

pip install kaggle

# ------ Define the directory of input "dataset" and create the directory if it does not exist,
#        thwn update the images and COCO Annotation.json

import os
dataset_dir = "/content/dataset"

# Create the directory if it does not exist
os.makedirs(dataset_dir, exist_ok=True)

# Commented out IPython magic to ensure Python compatibility.
import torch
from PIL import Image
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.datasets import CocoDetection
import matplotlib.pyplot as plt
# %matplotlib inline


COCO_CLASSES = {0: "_Background_", 1: "wine or spirit", 2: "water bottle", 3: "_other_"}
def get_class_name(class_id):
  return COCO_CLASSES.get(class_id, "Unknown")

NUM_EPOCHS_ = 5

# ------ Load Faster R-CNN with ResNet-50 backbone -----------------------
#
import torchvision.models.detection as detection

def get_model(num_classes):
  # Load pre-trained Faster R-CNN
  model = detection.fasterrcnn_resnet50_fpn(pretrained=True)

  # Get the number of input features for the classifier
  in_features = model.roi_heads.box_predictor.cls_score.in_features

  # Replace the pre-trained head with a new one
  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
  return model

# Initialize the model
#
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

num_classes = len(COCO_CLASSES) + 1  # background + number of wine classes
print(f'num_classes: {num_classes}')
model = get_model(num_classes)

model.to(device)
model

# Define optimizer and learning rate
#
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)
# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

"""Augment the Dataset
Apply Transformations:

Use augmentation techniques to artificially increase dataset size:
Flip images horizontally or vertically.
Rotate and crop images.
Adjust brightness, contrast, and hue.

"""

# ---------- Define augmentation transforms ------------
import os
import json
def augment_image(image, augmentation_type):
    if augmentation_type == "flip_h":
        return ImageOps.mirror(image)
    elif augmentation_type == "flip_v":
        return ImageOps.flip(image)
    elif augmentation_type == "rotate":
        return image.rotate(15, expand=True)
    elif augmentation_type == "color":
        color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2)
        return color_jitter(image)
    return image

# ---------- Function to process dataset subsets ------------------
def process_subset(images_subset, subset_path, annotations, augment=False):
    images_folder = os.path.join(subset_path, "images")
    annotation_file = os.path.join(subset_path, "annotations.json")

    subset_data = {"images": [], "annotations": [], "categories": categories}
    image_id_map = {}

    new_image_id = max(img["id"] for img in images) + 1
    new_annotation_id = max(ann["id"] for ann in annotations) + 1

    for img in images_subset:
        img_src = os.path.join(dataset_path, img["file_name"])
        if not os.path.exists(img_src):
            continue

        # Copy original image
        img_dest = os.path.join(images_folder, img["file_name"])
        shutil.copy(img_src, img_dest)

        # Assign new image ID
        old_image_id = img["id"]
        img["id"] = new_image_id
        subset_data["images"].append(img.copy())
        image_id_map[old_image_id] = new_image_id

        # Copy corresponding annotations
        for ann in annotations:
            if ann["image_id"] == old_image_id:
                new_ann = ann.copy()
                new_ann["image_id"] = new_image_id
                new_ann["id"] = new_annotation_id
                subset_data["annotations"].append(new_ann)
                new_annotation_id += 1

        # Process augmentations if required
        if augment:
            with Image.open(img_src).convert("RGB") as image:
                for aug_type in ["flip_h", "flip_v", "rotate", "color"]:
                    aug_img = augment_image(image, aug_type)
                    aug_filename = f"{os.path.splitext(img['file_name'])[0]}_{aug_type}.jpg"
                    aug_img_path = os.path.join(images_folder, aug_filename)
                    aug_img.save(aug_img_path)

                    # Create new image entry
                    aug_img_data = img.copy()
                    aug_img_data["id"] = new_image_id
                    aug_img_data["file_name"] = aug_filename
                    subset_data["images"].append(aug_img_data)

                    # Create new annotation entries for the augmented image
                    for ann in annotations:
                        if ann["image_id"] == old_image_id:
                            aug_ann = ann.copy()
                            aug_ann["id"] = new_annotation_id
                            aug_ann["image_id"] = new_image_id
                            subset_data["annotations"].append(aug_ann)
                            new_annotation_id += 1

                    new_image_id += 1  # Increment for next augmented image

        new_image_id += 1  # Increment for next original image

    # Save new annotations
    with open(annotation_file, "w") as f:
        json.dump(subset_data, f, indent=4)

    return subset_data["annotations"]

# -------- Function to augment images --------
def augment_image(image, aug_type):
    """Apply different augmentations to an image."""
    if aug_type == "flip_h":
        return ImageOps.mirror(image)
    elif aug_type == "flip_v":
        return ImageOps.flip(image)
    elif aug_type == "rotate":
        return image.rotate(90)
    elif aug_type == "color":
        return image.convert("L")  # Convert to grayscale as an example
    return image

#----------- Augment the Dataset and split dataset and its Augmented into train/test/val ------------
import os
import json
import random
import shutil
from PIL import Image, ImageOps

# Paths
dataset_path = "/content/dataset"
train_path = "/content/train"
val_path = "/content/val"
test_path = "/content/test"

# Create directories
for path in [train_path, val_path, test_path]:
    os.makedirs(os.path.join(path, "images"), exist_ok=True)

# Load annotation file
annotations_file = os.path.join(dataset_path, "annotations.json")
with open(annotations_file, "r") as f:
    coco_data = json.load(f)

images = coco_data["images"]
annotations = coco_data["annotations"]
categories = coco_data["categories"]

# Shuffle images
random.shuffle(images)
train_split = int(0.7 * len(images))
val_split = int(0.9 * len(images))

# Process subsets
train_annotations = process_subset(images[:train_split], train_path, annotations, augment=True)
val_annotations = process_subset(images[train_split:val_split], val_path, annotations, augment=False)
test_annotations = process_subset(images[val_split:], test_path, annotations, augment=False)

print("Dataset augmentation and split completed!")



# ----------- create CustomDataset() ---------------------
#
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import os
import json  # Import json for loading annotations

class CustomDataset(Dataset):
    def __init__(self, root_dir, annotations_file, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]

        # Load annotations from JSON file
        with open(annotations_file, 'r') as f:
            annotations_data = json.load(f)

        # Create a dictionary mapping image filenames to annotations
        self.image_annotations = {}
        for annotation in annotations_data['annotations']:
            # Get the image filename from the annotation's image_id
            image_info = next((img for img in annotations_data['images'] if img['id'] == annotation['image_id']), None)

            if image_info:
                image_filename = image_info['file_name']
                if image_filename not in self.image_annotations:
                    self.image_annotations[image_filename] = []
                self.image_annotations[image_filename].append(annotation)

        # Create a list to store annotations for each image file
        self.annotations = []
        for image_file in self.image_files:
            # Get annotations for this image from the dictionary using filename as key
            image_annotations = self.image_annotations.get(image_file, [])

            # Create a target dictionary
            target = {
                'boxes': [],
                'labels': []
            }
            for annotation in image_annotations:
                bbox = annotation['bbox']
                # COCO format is [x, y, width, height], convert to [x1, y1, x2, y2]
                target['boxes'].append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])
                target['labels'].append(annotation['category_id'])

            # Convert boxes and labels to tensors
            target['boxes'] = torch.tensor(target['boxes'], dtype=torch.float32)
            target['labels'] = torch.tensor(target['labels'], dtype=torch.int64)

            self.annotations.append(target)

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, self.image_files[idx])
        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        target = self.annotations[idx]
        # modified by dv
        # Ensure target labels are within the expected range
        target['labels'] = torch.clamp(target['labels'], 0, num_classes - 1)

        return image, target

def fix_bounding_boxes(targets):
    fixed_targets = []
    for t in targets:
        valid_boxes = []
        valid_labels = []

        for box, label in zip(t["boxes"], t["labels"]):
            x_min, y_min, x_max, y_max = box
            width, height = x_max - x_min, y_max - y_min

            # Ensure bbox has positive width & height
            if width > 0 and height > 0:
                valid_boxes.append([x_min, y_min, x_max, y_max])
                valid_labels.append(label)

        # Update target if valid boxes exist
        if valid_boxes:
            t["boxes"] = torch.tensor(valid_boxes, dtype=torch.float32)
            t["labels"] = torch.tensor(valid_labels, dtype=torch.int64)
            fixed_targets.append(t)

    return fixed_targets

# ------------ Custom collate function to handle varying number of bounding boxes
#
import torch
import torch.nn.functional as F

def collate_fn(batch):
    images = []
    targets = []

    # Find the max size for padding
    max_height = max([item[0].size(1) for item in batch])  # item[0] is the image
    max_width = max([item[0].size(2) for item in batch])   # item[0] is the image

    for item in batch:
        image = item[0]  # Access image correctly
        target = item[1]  # Assuming target is the second element

        # Calculate padding for each image
        padding = (0, max_width - image.size(2), 0, max_height - image.size(1))  # (left, right, top, bottom)
        image = F.pad(image, padding, value=0)  # Pad with zeros

        images.append(image)
        targets.append(target)

    # Stack images
    images = torch.stack(images, dim=0)

    # Create lists to store boxes and labels from all targets
    all_boxes = [t['boxes'] for t in targets]
    all_labels = [t['labels'] for t in targets]

    # Pad boxes and labels to the maximum length within the batch
    max_num_boxes = max([len(boxes) for boxes in all_boxes])
    padded_boxes = [torch.cat([boxes, torch.zeros(max_num_boxes - len(boxes), 4)], dim=0) for boxes in all_boxes]
    padded_labels = [torch.cat([labels, torch.zeros(max_num_boxes - len(labels), dtype=torch.int64)], dim=0) for labels in all_labels]

    # Stack padded boxes and labels to create tensors
    boxes_tensor = torch.stack(padded_boxes, dim=0)
    labels_tensor = torch.stack(padded_labels, dim=0)

    # Create a list of dictionaries with the padded tensors
    targets = [{'boxes': boxes, 'labels': labels} for boxes, labels in zip(padded_boxes, padded_labels)]

    return images, targets

"""Train the Model:

[i]. Use a training loop to fine-tune the model with your dataset.

[ii]. Training loop:
"""

# ---------------- load train/val dataset to the memory -------------
import torch
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import os
import json  # Import json for loading annotations

# Load Dataset
transform = transforms.Compose([
    transforms.ToTensor(),
])

# Assuming your annotations JSON file is in the same directory as the images
annotations_file = '/content/train/annotations.json'
dataset_train = CustomDataset('/content/train/images', annotations_file, transform=transform)

annotations_file_test = '/content/test/annotations.json'
dataset_test = CustomDataset('/content/test/images', annotations_file, transform=transform)

annotations_file_val = '/content/val/annotations.json'
dataset_val = CustomDataset('/content/val/images', annotations_file, transform=transform)

# load train/val dataset to the memory.
#
# data_loader = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=collate_fn)
train_dataLoader = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=collate_fn)
val_dataLoader = DataLoader(dataset_val, batch_size=4, shuffle=False, collate_fn=collate_fn)
test_dataLoader = DataLoader(dataset_test, batch_size=4, shuffle=False, collate_fn=collate_fn)

img, target = dataset_train[0]
print(target)

for images, targets in train_dataLoader:
    print(targets)  # Check if annotations are properly loaded
    break

num_samples_train = len(train_dataLoader.dataset)
num_samples_test = len(test_dataLoader.dataset)
num_samples_val = len(val_dataLoader.dataset)
print(f"Total number of samples: [Train]: {num_samples_train}, [Test]: {num_samples_test}, [val]:  {num_samples_val}")

# ---------- Train one epoch --------------------------------------
#
def train_Epoch(model, optimizer, data_loader, device, epoch):
    model.train()
    epoch_loss = 0

    for images, targets in data_loader:
        images = [img.to(device) for img in images]

        valid_targets = []
        valid_images = []

        for i, target in enumerate(targets):
            boxes = target['boxes']
            labels = target['labels']

            # Remove invalid bounding boxes (zero-area and empty)
            valid_idx = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])  # x_max > x_min and y_max > y_min
            boxes = boxes[valid_idx]
            labels = labels[valid_idx]

            if len(boxes) > 0:  # Keep only valid samples
                valid_targets.append({"boxes": boxes.to(device), "labels": labels.to(device)})
                valid_images.append(images[i])

        if not valid_targets:  # If no valid targets in batch, skip iteration
            continue

        # Forward pass
        loss_dict = model(valid_images, valid_targets)
        losses = sum(loss for loss in loss_dict.values())

        epoch_loss += losses.cpu().detach().numpy()

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

    print(f'Epoch: [{epoch}], Loss: {(epoch_loss / len(data_loader)):.4f}')

"""# Step 5: Evaluation Metrics."""

# ---------------- Evaluation Function ---------------
#
import torch
import torchvision
from torchvision.ops import box_iou
from collections import defaultdict
import numpy as np

def evaluate_model(model, data_loader, device, iou_thresholds=[0.5, 0.75]):
    """
    Evaluate Faster R-CNN model on validation data using mAP and IoU.
    """
    model.eval()
    results = defaultdict(list)

    with torch.no_grad():
        for images, targets in data_loader:
            images = [img.to(device) for img in images]
            outputs = model(images)  # Get predictions

            for i, output in enumerate(outputs):
                pred_boxes = output['boxes'].cpu().numpy()
                pred_scores = output['scores'].cpu().numpy()
                pred_labels = output['labels'].cpu().numpy()

                true_boxes = targets[i]['boxes'].cpu().numpy()
                true_labels = targets[i]['labels'].cpu().numpy()

                # Handle case where there are no predictions
                if len(pred_boxes) == 0:
                    if len(true_boxes) == 0:
                        # No GT and no predictions → Perfect precision & recall
                        results['precision'].append(1)
                        results['recall'].append(1)
                    else:
                        # No predictions but there were ground truths → Worst case
                        results['precision'].append(0)
                        results['recall'].append(0)
                    results['ap'].append(0)
                    continue

                # Handle case where there are no ground truth boxes
                if len(true_boxes) == 0:
                    results['precision'].append(1)
                    results['recall'].append(1)
                    results['ap'].append(0)
                    continue

                # Compute IoU between predictions and ground truth
                ious = box_iou(torch.tensor(pred_boxes), torch.tensor(true_boxes))

                ap_scores = []
                for iou_thresh in iou_thresholds:
                    matches = ious > iou_thresh  # IoU matches at threshold

                    tp = (matches.sum(dim=1) > 0).sum().item()  # Count matched predictions
                    fp = len(pred_boxes) - tp  # False Positives
                    fn = len(true_boxes) - tp  # False Negatives

                    precision = tp / (tp + fp + 1e-6)
                    recall = tp / (tp + fn + 1e-6)

                    ap_scores.append(precision * recall)  # Approximate AP calculation

                avg_ap = np.mean(ap_scores)  # Mean AP across IoU thresholds

                results['precision'].append(precision)
                results['recall'].append(recall)
                results['ap'].append(avg_ap)

    # Compute overall averages
    avg_precision = np.mean(results['precision'])
    avg_recall = np.mean(results['recall'])
    mean_ap = np.mean(results['ap'])

    print(f" Model Evaluation:\nPrecision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, mAP: {mean_ap:.4f}")

    return avg_precision, avg_recall, mean_ap

# ------ start full training without evaluation function ----------
#
""""
import os
import torch

# Define the directory
model_dir = "/content/trained_models"

# Create the directory if it does not exist
os.makedirs(model_dir, exist_ok=True)


# Training loop
#----------------------------
num_epochs = NUM_EPOCHS_

for epoch in range(num_epochs):
  train_Epoch(model, optimizer, train_dataLoader, device, epoch)

  # Save the model
  model_path = f"{model_dir}/fasterrcnn_resnet50_epoch_{epoch + 1}.pth"
  torch.save(model.state_dict(), model_path)
  print(f"Model saved: {model_path}")

"""

"""
# Training loop with evaluation
"""

import os
import torch

# Define the directory
model_dir = "/content/trained_models"

# Create the directory if it does not exist
os.makedirs(model_dir, exist_ok=True)

num_epochs = NUM_EPOCHS_

for epoch in range(num_epochs):
    train_Epoch(model, optimizer, train_dataLoader, device, epoch)

    # Evaluate model after each epoch
    print("\n Evaluating model...")
    avg_precision, avg_recall, mean_ap = evaluate_model(model, val_dataLoader, device)

    # Save the model
    model_path = f"{model_dir}/fasterrcnn_resnet50_epoch_{epoch + 1}.pth"
    torch.save(model.state_dict(), model_path)
    print(f" Model saved: {model_path}")



"""Evaluate and Visualize Metrics:

Calculate mAP, Precision, and Recall using tools like TorchMetrics. Visualize Results:

"""

# .......... evaluate .....
model.eval()
data = iter(val_dataLoader).__next__()
img, target = data  # If target is a list of one dictionary
target = target[0]  # now it should be a dictionary
boxes = target['boxes']
labels = target['labels']

print(type(data))  # Check if it's a tuple or a tensor
print(len(data))   # If it's a tuple/list, check its length
print(data[0].shape)  # Shape of the first item

img = data[0][0]

output = model([img.to(device)])
output[0].keys()
boxes, labels, scores = output[0]["boxes"], output[0]["labels"], output[0]["scores"]
scores

def visualize_predictions(image, predictions, threshold):
    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):
        if score > threshold:  # Confidence threshold
            class_name = get_class_name(label.item())
            print(f'class_name: {class_name}, label: {label.item()}')
            x1, y1, x2, y2 = map(int, box.tolist())
            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)
            cv2.putText(img, f'{class_name} {score:.2f}', (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    plt.imshow(img)
    plt.axis('off')
    plt.show()

# ------------ show the result on test subfolder -------------------
#
import cv2
from torchvision.io import read_image
import matplotlib.pyplot as plt
import os
import glob

def load_image(image_path):
    image = read_image(image_path)  # Loads as a tensor
    image = image.float() / 255.0  # Normalize to [0,1] if required
    return image

# Search for JPG files in the directory
test_path = "/content/test/images"
jpg_files = glob.glob(os.path.join(test_path, "*.jpg"))

if jpg_files:
    image_path = jpg_files[0]  # Pick the first JPG file
    print(f"Processing file: {image_path}")

    # Further processing can be done here
    # Inference
    # Ensure model is in evaluation mode
    model.eval()

    # Move image to device
    image = load_image(image_path).to(device)

    # Run inference
    with torch.no_grad():  # Disable gradient calculation
        predictions = model([image])[0]  # Pass as a list

    # Print predictions for debugging
    print(predictions)

    pred_prob = torch.softmax(predictions['scores'], dim=0)

    # Visualization
    threshold = 0.50
    image_cv = cv2.imread(image_path)
    visualize_predictions(image_cv, predictions, threshold)
    pred_prob[:2]

else:
    print("No JPG file found")



"""# Load the existing trained model and test unseen images
(e.g. fasterrcnn_resnet50_epoch_3.pth)
"""

from torchvision import transforms

def prepare_image(image_path):
    image = Image.open(image_path).convert("RGB")
    transform = transforms.ToTensor()  # Define transformation
    image_tensor = transform(image).unsqueeze(0)  # Convert image to tensor and add batch dimension
    return image_tensor.to(device)



import matplotlib.pyplot as plt  # Import matplotlib
# -------------- Draw bounding box with the correct class names and image size ----------
#
def draw_boxes(image, prediction, fig_size=(10, 10)):
  # get the predicted bbox, labels and scores
  boxes = prediction[0]['boxes'].cpu().numpy()
  labels = prediction[0]['labels'].cpu().numpy()
  scores = prediction[0]['scores'].cpu().numpy()

  # Set a threshold for showing boxes
  threshold = 0.7

  # Setup the figure size to control the image size
  plt.figure(figsize=fig_size)

  for box, label, score in zip(boxes, labels, scores):
    # print(f'score: {score}')
    if (score) > threshold:
      x_min, y_min, x_max, y_max = box
      name = get_class_name(label)
      print(f'score: {score}, label name: {name}, box: {x_min}, {y_min}, {x_max}, {y_max}')
      class_name = get_class_name(label)
      plt.imshow(image)

      plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max-x_min, y_max-y_min,
                    linewidth=2, edgecolor='r', facecolor='none'))

      plt.text(x_min, y_min, f"{class_name} ({score:.2f})", color='b')
      plt.axis('off')
      plt.show()

import torch
# ------------- Load the trained model --------------------

model = get_model(num_classes)
model.load_state_dict(torch.load("/content/trained_models/fasterrcnn_resnet50_epoch_3.pth"))
model.to(device)
model.eval()  # set the model to evaluation model

# ----------- Load the unseen image and show the reuslt ---------------------------
# Search for JPG files in the directory
test_path = "/content/val/images"

jpg_files = glob.glob(os.path.join(test_path, "*.jpg"))

if jpg_files:
    for image_path in jpg_files:

      print(f"test file: {image_path}")
      #....................
      image_tensor = prepare_image(image_path)

      with torch.no_grad():   # Disable gradient computation for interence
        prediction = model(image_tensor)

      # display the image with bounding boxes and correct labels
      image = Image.open(image_path)  # Open the image correctly
      draw_boxes(image, prediction, fig_size=(12, 10))

# ----------- Load one unseen image and show its reuslt ---------------------------
test_path = "/content/train/images"
test_image = "100001.jpg"

# ---------------------------------------
image_path = os.path.join(test_path, test_image)
image_tensor = prepare_image(image_path)

with torch.no_grad():   # Disable gradient computation for interence
  prediction = model(image_tensor)[0]

# display the image with bounding boxes and correct labels
threshold = 0.7
image_cv = cv2.imread(image_path)
visualize_predictions(image_cv, prediction, threshold)
prediction

############################# test using original resnet50 model ################################
# using non-tuning model
test_path = "/content/test/images"
test_image = "100000.jpg"
image_path = os.path.join(test_path, test_image)

model_ = detection.fasterrcnn_resnet50_fpn(pretrained=True)

image_tensor = prepare_image(image_path)

model_.eval()

with torch.no_grad():   # Disable gradient computation for interence
  prediction = model_(image_tensor)[0]

# display the image with bounding boxes and correct labels
threshold = 0.8
image_cv = cv2.imread(image_path)
visualize_predictions(image_cv, prediction, threshold)

prediction

